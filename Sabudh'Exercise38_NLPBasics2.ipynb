{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "import nltk.corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abc', 'abc.zip', 'alpino', 'alpino.zip', 'biocreative_ppi', 'biocreative_ppi.zip', 'brown', 'brown.zip', 'brown_tei', 'brown_tei.zip', 'cess_cat', 'cess_cat.zip', 'cess_esp', 'cess_esp.zip', 'chat80', 'chat80.zip', 'city_database', 'city_database.zip', 'cmudict', 'cmudict.zip', 'comparative_sentences', 'comparative_sentences.zip', 'comtrans.zip', 'conll2000', 'conll2000.zip', 'conll2002', 'conll2002.zip', 'conll2007.zip', 'crubadan', 'crubadan.zip', 'dependency_treebank', 'dependency_treebank.zip', 'dolch', 'dolch.zip', 'europarl_raw', 'europarl_raw.zip', 'floresta', 'floresta.zip', 'framenet_v15', 'framenet_v15.zip', 'framenet_v17', 'framenet_v17.zip', 'gazetteers', 'gazetteers.zip', 'genesis', 'genesis.zip', 'gutenberg', 'gutenberg.zip', 'ieer', 'ieer.zip', 'inaugural', 'inaugural.zip', 'indian', 'indian.zip', 'jeita.zip', 'kimmo', 'kimmo.zip', 'knbc.zip', 'lin_thesaurus', 'lin_thesaurus.zip', 'machado.zip', 'mac_morpho', 'mac_morpho.zip', 'masc_tagged.zip', 'movie_reviews', 'movie_reviews.zip', 'mte_teip5', 'mte_teip5.zip', 'names', 'names.zip', 'nombank.1.0.zip', 'nonbreaking_prefixes', 'nonbreaking_prefixes.zip', 'nps_chat', 'nps_chat.zip', 'omw', 'omw.zip', 'opinion_lexicon', 'opinion_lexicon.zip', 'panlex_swadesh.zip', 'paradigms', 'paradigms.zip', 'pil', 'pil.zip', 'pl196x', 'pl196x.zip', 'ppattach', 'ppattach.zip', 'problem_reports', 'problem_reports.zip', 'product_reviews_1', 'product_reviews_1.zip', 'product_reviews_2', 'product_reviews_2.zip', 'propbank.zip', 'pros_cons', 'pros_cons.zip', 'ptb', 'ptb.zip', 'qc', 'qc.zip', 'reuters.zip', 'rte', 'rte.zip', 'semcor.zip', 'senseval', 'senseval.zip', 'sentence_polarity', 'sentence_polarity.zip', 'sentiwordnet', 'sentiwordnet.zip', 'shakespeare', 'shakespeare.zip', 'sinica_treebank', 'sinica_treebank.zip', 'smultron', 'smultron.zip', 'state_union', 'state_union.zip', 'stopwords', 'stopwords.zip', 'subjectivity', 'subjectivity.zip', 'swadesh', 'swadesh.zip', 'switchboard', 'switchboard.zip', 'timit', 'timit.zip', 'toolbox', 'toolbox.zip', 'treebank', 'treebank.zip', 'twitter_samples', 'twitter_samples.zip', 'udhr', 'udhr.zip', 'udhr2', 'udhr2.zip', 'unicode_samples', 'unicode_samples.zip', 'universal_treebanks_v20.zip', 'verbnet', 'verbnet.zip', 'webtext', 'webtext.zip', 'wordnet', 'wordnet.zip', 'wordnet_ic', 'wordnet_ic.zip', 'words', 'words.zip', 'ycoe', 'ycoe.zip']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(nltk.data.find(\"corpora\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ai ='According to industry estimates, only 21% of the available data is present in structured form. Data is being generated as we speak, as we tweet, as we send messages on Whatsapp and in various other activities. Majority of this data exists in the textual form, which is highly unstructured in natureew notorious examples include â€“ tweets / posts on social media, user to user chat conversations, news, blogs and articles, product or services reviews and patient records in the healthcare sector. A few more recent ones includes chatbots and other voice driven bots.Despite having high dimension data, the information present in it is not directly accessible unless it is processed (read and understood) manually or analyzed by an automated system.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(Ai))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ai_tokenize = word_tokenize(Ai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ai_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Ai_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "Ai_frequency = FreqDist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({})"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ai_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({',': 9, 'in': 6, 'is': 5, 'and': 5, 'the': 4, 'data': 4, '.': 4, 'as': 3, 'we': 3, 'to': 2, ...})"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for word in Ai_tokenize:\n",
    "    Ai_frequency[word.lower()]+=1\n",
    "Ai_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 9),\n",
       " ('in', 6),\n",
       " ('is', 5),\n",
       " ('and', 5),\n",
       " ('the', 4),\n",
       " ('data', 4),\n",
       " ('.', 4),\n",
       " ('as', 3),\n",
       " ('we', 3),\n",
       " ('to', 2)]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ai_frequency.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import blankline_tokenize# for getting the [how many pharagraph are there in the give data]\n",
    "Ai_blackline = blankline_tokenize(Ai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Ai_blackline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import bigrams,trigrams,ngrams\n",
    "\n",
    "st = \"This guide unearths the concepts of natural language processing, its techniques and implementation\"\n",
    "st_token = nltk.word_tokenize(st)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', 'guide'),\n",
       " ('guide', 'unearths'),\n",
       " ('unearths', 'the'),\n",
       " ('the', 'concepts'),\n",
       " ('concepts', 'of'),\n",
       " ('of', 'natural'),\n",
       " ('natural', 'language'),\n",
       " ('language', 'processing'),\n",
       " ('processing', ','),\n",
       " (',', 'its'),\n",
       " ('its', 'techniques'),\n",
       " ('techniques', 'and'),\n",
       " ('and', 'implementation')]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st_bigram = list(nltk.bigrams(st_token))\n",
    "st_bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', 'guide', 'unearths'),\n",
       " ('guide', 'unearths', 'the'),\n",
       " ('unearths', 'the', 'concepts'),\n",
       " ('the', 'concepts', 'of'),\n",
       " ('concepts', 'of', 'natural'),\n",
       " ('of', 'natural', 'language'),\n",
       " ('natural', 'language', 'processing'),\n",
       " ('language', 'processing', ','),\n",
       " ('processing', ',', 'its'),\n",
       " (',', 'its', 'techniques'),\n",
       " ('its', 'techniques', 'and'),\n",
       " ('techniques', 'and', 'implementation')]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st_trigram = list(nltk.trigrams(st_token))\n",
    "st_trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', 'guide', 'unearths', 'the'),\n",
       " ('guide', 'unearths', 'the', 'concepts'),\n",
       " ('unearths', 'the', 'concepts', 'of'),\n",
       " ('the', 'concepts', 'of', 'natural'),\n",
       " ('concepts', 'of', 'natural', 'language'),\n",
       " ('of', 'natural', 'language', 'processing'),\n",
       " ('natural', 'language', 'processing', ','),\n",
       " ('language', 'processing', ',', 'its'),\n",
       " ('processing', ',', 'its', 'techniques'),\n",
       " (',', 'its', 'techniques', 'and'),\n",
       " ('its', 'techniques', 'and', 'implementation')]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st_ngrams = list(nltk.ngrams(st_token,4))\n",
    "st_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# once we have token now we can go toward stemming which normalize word into their root word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "pst  = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give\n",
      "give\n",
      "given\n",
      "gave\n"
     ]
    }
   ],
   "source": [
    "word_to_stem = [\"give\",\"giving\",\"given\",\"gave\"]\n",
    "for word in word_to_stem:\n",
    "    print(pst.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "lst  = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "giv\n",
      "giv\n",
      "giv\n",
      "gav\n"
     ]
    }
   ],
   "source": [
    "word_to_stem = [\"give\",\"giving\",\"given\",\"gave\"]\n",
    "for word in word_to_stem:\n",
    "    print(lst.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the Lemmatization in this process we will have output a proper word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wlm = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'put'"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wlm.lemmatize(\"put\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "len(stopwords.words(\"English\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['according', 'industry', 'estimates,', '21%', 'available', 'data', 'present', 'structured', 'form.', 'data', 'generated', 'speak,', 'tweet,', 'send', 'messages', 'whatsapp', 'various', 'activities.', 'majority', 'data', 'exists', 'textual', 'form,', 'highly', 'unstructured', 'natureew', 'notorious', 'examples', 'include', 'â€“', 'tweets', '/', 'posts', 'social', 'media,', 'user', 'user', 'chat', 'conversations,', 'news,', 'blogs', 'articles,', 'product', 'services', 'reviews', 'patient', 'records', 'healthcare', 'sector.', 'recent', 'ones', 'includes', 'chatbots', 'voice', 'driven', 'bots.despite', 'high', 'dimension', 'data,', 'information', 'present', 'directly', 'accessible', 'unless', 'processed', '(read', 'understood)', 'manually', 'analyzed', 'automated', 'system.']\n"
     ]
    }
   ],
   "source": [
    "stop = stopwords.words(\"English\")\n",
    "print([i for i in Ai.lower().split() if i not in stop]) # remove stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now Parts of Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"Gaurav is doing internship in sabudh foundation\"\n",
    "sent_tokenize = word_tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Gaurav', 'NN')]\n",
      "[('is', 'VBZ')]\n",
      "[('doing', 'VBG')]\n",
      "[('internship', 'NN')]\n",
      "[('in', 'IN')]\n",
      "[('sabudh', 'NN')]\n",
      "[('foundation', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "for token in sent_tokenize:\n",
    "    print(nltk.pos_tag([token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now comes Named entity recongiztion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ne_chunk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "ne_sent = \"india will become ai leader\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "ne_tokenize = word_tokenize(ne_sent)\n",
    "ne_tag = nltk.pos_tag(ne_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S india/NN will/MD become/VB ai/JJ leader/NN)\n"
     ]
    }
   ],
   "source": [
    "ne_output  = ne_chunk(ne_tag)\n",
    "print(ne_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
